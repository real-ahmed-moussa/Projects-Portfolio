{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insurance Policy Binary Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2] Load and Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whole Dataset\n",
    "data = pd.read_csv(\"TrainingDataset_2023Qualification.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Features and Target Variables\n",
    "features = data.iloc[:, 2:]\n",
    "target = data.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Data into Training and Testing Sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3] Exploratory Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3.1] Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Data Types\n",
    "x_train.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Numerical Continuous Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Numerical Continuous Variables for Anomalies [Outliers]\n",
    "x_train[[\"policyHolderAge\", \"homeInsurancePremium\", \"nbWeeksInsured\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a Histogram of the Numerical Variables\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "ax[0].hist(x_train[\"policyHolderAge\"], bins=20, color='blue')\n",
    "ax[0].set_title('policyHolderAge')\n",
    "ax[1].hist(x_train[\"homeInsurancePremium\"], bins=20, color='red')\n",
    "ax[1].set_title('homeInsurancePremium')\n",
    "ax[2].hist(x_train[\"nbWeeksInsured\"], bins=20, color='green')\n",
    "ax[2].set_title('nbWeeksInsured')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a Scatterplot of the Numerical Variables\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "ax[0].scatter(x_train[\"policyHolderAge\"], x_train[\"homeInsurancePremium\"], color='blue')\n",
    "ax[0].set_title('policyHolderAge vs. homeInsurancePremium')\n",
    "ax[1].scatter(x_train[\"policyHolderAge\"], x_train[\"nbWeeksInsured\"], color='red')\n",
    "ax[1].set_title('policyHolderAge vs. nbWeeksInsured')\n",
    "ax[2].scatter(x_train[\"homeInsurancePremium\"], x_train[\"nbWeeksInsured\"], color='green')\n",
    "ax[2].set_title('homeInsurancePremium vs. nbWeeksInsured')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct for policyHolderAge and homeInurancePremium Skewness\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax[0].hist(np.log(x_train[\"policyHolderAge\"]), bins=25, color='blue')\n",
    "ax[0].set_title('policyHolderAge')\n",
    "\n",
    "ax[1].hist(np.log(x_train[\"homeInsurancePremium\"]), bins=25, color='red')\n",
    "ax[1].set_title('homeInsurancePremium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Outliers for homeInsurancePremium\n",
    "\"\"\" Outliers do exist within the homeInsurancePremium variable. However, having more than one outlier means they did not come by mistake. Hence, they will be kept but with caution. \"\"\"\n",
    "mask = x_train[\"homeInsurancePremium\"] > 4000\n",
    "print(mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Colinearity\n",
    "x_train[[\"policyHolderAge\", \"homeInsurancePremium\", \"nbWeeksInsured\"]].corr()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Categorical Variables - High Cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One can use: frequency encoding, target encoding, hashing trick encoding, or embeding\n",
    "# Check the Number of Distinct Values - Variables are already Label Encoded!\n",
    "x_train[[\"territory\", \"saleChannel\"]].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a Histogram of the Variables\n",
    "ig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax[0].hist(x_train[\"territory\"], bins=20, color='blue')\n",
    "ax[0].set_title('territory')\n",
    "ax[1].hist(x_train[\"saleChannel\"], bins=20, color='red')\n",
    "ax[1].set_title('saleChannel')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Categorical Dependence\n",
    "# Comment: Some p-values are zero which indicates there is a relationship between their corresponding variables.\n",
    "#          A low p-value doesn't necessarily mean the relationship is strong, and it doesn't provide information about the nature or strength of the relationship.\n",
    "#          In addition, a low p-value could also mean that the sample size is large enough to detect even small differences between observed and expected frequencies.\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "contingency_table = pd.crosstab(index=x_train[\"territory\"], columns=x_train[\"saleChannel\"])\n",
    "stat, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(\"Chi-square Statistic is: \" + str(stat))\n",
    "print(\"p-value is: \" + str(p))\n",
    "print(\"Number of DOFs is: \" + str(dof))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Categorical Variables - Low Cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Frequencies of Categorical Variables\n",
    "print(x_train[[\"Gender\"]].value_counts())\n",
    "print(\"---------\")\n",
    "print(x_train[[\"hasCanadianDrivingLicense\" ]].value_counts())\n",
    "print(\"---------\")\n",
    "print(x_train[[\"hasAutoInsurance\"]].value_counts())\n",
    "print(\"---------\")\n",
    "print(x_train[[\"hadVehicleClaimInPast\"]].value_counts())\n",
    "print(\"---------\")\n",
    "print(x_train[[\"isOwner\"]].value_counts())\n",
    "print(\"---------\")\n",
    "print(x_train[[\"rentedVehicle\"]].value_counts())\n",
    "print(\"---------\")\n",
    "print(x_train[[\"hasMortgage\"]].value_counts())\n",
    "print(\"---------\")\n",
    "print(x_train[[\"vehicleStatus\"]].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a Histogram of the Variables\n",
    "columns = ['Gender', 'hasCanadianDrivingLicense', \"hasAutoInsurance\", \"hadVehicleClaimInPast\", \"isOwner\", \"rentedVehicle\", \"hasMortgage\", \"vehicleStatus\"]\n",
    "\n",
    "# set the number of rows and columns for the subplot\n",
    "nrows = 2\n",
    "ncols = 4\n",
    "\n",
    "# create the subplot grid\n",
    "fig, ax = plt.subplots(nrows, ncols, figsize=(16, 8))\n",
    "\n",
    "# loop through the columns and axes\n",
    "for i, col in enumerate(columns):\n",
    "    # calculate the frequency of each category in the column\n",
    "    freq = x_train[col].value_counts()\n",
    "    \n",
    "    # get the axis for the subplot\n",
    "    axi = ax.flat[i]\n",
    "    \n",
    "    # plot the frequency as a bar chart\n",
    "    freq.plot(kind='bar', ax=axi)\n",
    "    axi.set_title(col)\n",
    "    axi.set_xlabel(col)\n",
    "    axi.set_ylabel('Frequency')\n",
    "\n",
    "# adjust the layout of the subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Dependence\n",
    "# [1] Establish contingency tables\n",
    "# [2] Perform Chi-squared test\n",
    "# [3] Comment: Some p-values are zero which indicates there is a relationship between their corresponding variables.\n",
    "#              A low p-value doesn't necessarily mean the relationship is strong, and it doesn't provide information about the nature or strength of the relationship.\n",
    "#              In addition, a low p-value could also mean that the sample size is large enough to detect even small differences between observed and expected frequencies.\n",
    "#              Proceed with caution!\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "p_matrix = -1 * np.ones((len(columns), len(columns)))\n",
    "\n",
    "for i in range(len(columns)):\n",
    "    for j in range(i+1, len(columns)):\n",
    "        \n",
    "        contingency_table = pd.crosstab(index=x_train[columns[i]], columns=x_train[columns[j]])\n",
    "        stat, p, dof, expected = chi2_contingency(contingency_table)\n",
    "        \n",
    "        p_matrix[i, j] = np.round(p, 5)\n",
    "        \n",
    "print(p_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4 Response Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Response Variable for Class Imbalance\n",
    "target_imbalance = (y_train.value_counts()/x_train.shape[0]) * 100\n",
    "target_imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Classes\n",
    "frequency = y_train.value_counts()\n",
    "\n",
    "# Plot the frequency using a bar plot\n",
    "frequency.plot(kind='bar')\n",
    "\n",
    "# Add labels to the x and y axes\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3.2] Check for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NA Values in \"hasMortgage\" means the house is rented. Hence, it needs to be imputed as a third category!\n",
    "x_train.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3.3] Data Pre-processing Pipelines\n",
    "Create a set of data preprocessing pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Columns Lists\n",
    "num_features = [\"policyHolderAge\", \"homeInsurancePremium\", \"nbWeeksInsured\"]\n",
    "cat_long_features = [\"territory\", \"saleChannel\"]\n",
    "cat_features = ['Gender', 'hasCanadianDrivingLicense', \"hasAutoInsurance\", \"hadVehicleClaimInPast\", \"isOwner\", \"rentedVehicle\", \"hasMortgage\", \"vehicleStatus\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transformer #1\n",
    "Imputation + Categorical Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_and_onehot_pipe = Pipeline(steps=[\n",
    "                                            ('imputer', SimpleImputer(strategy='constant', fill_value=2)),\n",
    "                                            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "                                        ])\n",
    "\n",
    "\n",
    "preprocessing_transformer1 = ColumnTransformer(transformers=[\n",
    "                                                                ('genderPipe', OneHotEncoder(handle_unknown='ignore'), [\"Gender\"]),\n",
    "                                                                ('vehicleClaimPipe', OneHotEncoder(handle_unknown='ignore'), [\"hadVehicleClaimInPast\"]),\n",
    "                                                                ('hasMortgagePipe', impute_and_onehot_pipe, [\"hasMortgage\"]),\n",
    "                                                                ('vehicleStatusPipe', OneHotEncoder(handle_unknown='ignore'), [\"vehicleStatus\"])\n",
    "                                                            ], remainder='passthrough')\n",
    "\n",
    "preprocessing_pipeline1 = Pipeline([\n",
    "                                      ('transform_column', preprocessing_transformer1)\n",
    "                                   ])\n",
    "preprocessing_pipeline1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transformer #2\n",
    "Imputation + Categorical Encodings + Numeric Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_and_onehot_pipe = Pipeline(steps=[\n",
    "                                            ('imputer', SimpleImputer(strategy='constant', fill_value=2)),\n",
    "                                            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "                                        ])\n",
    "\n",
    "\n",
    "preprocessing_transformer2 = ColumnTransformer(transformers=[\n",
    "                                                                (\"numericPipe_log\", FunctionTransformer(np.log, validate=True), [\"policyHolderAge\", \"homeInsurancePremium\"]),\n",
    "                                                                (\"numericPipe\", StandardScaler(), num_features),\n",
    "                                                                ('genderPipe', OneHotEncoder(handle_unknown='ignore'), [\"Gender\"]),\n",
    "                                                                ('vehicleClaimPipe', OneHotEncoder(handle_unknown='ignore'), [\"hadVehicleClaimInPast\"]),\n",
    "                                                                ('hasMortgagePipe', impute_and_onehot_pipe, [\"hasMortgage\"]),\n",
    "                                                                ('vehicleStatusPipe', OneHotEncoder(handle_unknown='ignore'), [\"vehicleStatus\"])\n",
    "                                                            ], remainder='passthrough')\n",
    "\n",
    "preprocessing_pipeline2 = Pipeline([\n",
    "                                      ('transform_column', preprocessing_transformer2)\n",
    "                                   ])\n",
    "preprocessing_pipeline2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transformer #3\n",
    "Imputation + Categorical Encodings + Numeric and Cat_long Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "impute_and_onehot_pipe = Pipeline(steps=[\n",
    "                                            ('imputer', SimpleImputer(strategy='constant', fill_value=2)),\n",
    "                                            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "                                        ])\n",
    "\n",
    "\n",
    "preprocessing_transformer3 = ColumnTransformer(transformers=[\n",
    "                                                                (\"numericPipe_log\", FunctionTransformer(np.log, validate=True), [\"policyHolderAge\", \"homeInsurancePremium\"]),\n",
    "                                                                (\"numericPipe\", StandardScaler(), num_features),\n",
    "                                                                (\"catLongPipe\", StandardScaler(), cat_long_features),\n",
    "                                                                ('genderPipe', OneHotEncoder(handle_unknown='ignore'), [\"Gender\"]),\n",
    "                                                                ('vehicleClaimPipe', OneHotEncoder(handle_unknown='ignore'), [\"hadVehicleClaimInPast\"]),\n",
    "                                                                ('hasMortgagePipe', impute_and_onehot_pipe, [\"hasMortgage\"]),\n",
    "                                                                ('vehicleStatusPipe', OneHotEncoder(handle_unknown='ignore'), [\"vehicleStatus\"]),\n",
    "                                                            ], remainder='passthrough')\n",
    "\n",
    "preprocessing_pipeline3 = Pipeline([\n",
    "                                      ('transform_column', preprocessing_transformer3)\n",
    "                                   ])\n",
    "preprocessing_pipeline3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transformer #4\n",
    "Imputation + Categorical Encodings + Numeric Standardization + Count Encoding (cat_long_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "impute_and_onehot_pipe = Pipeline(steps=[\n",
    "                                            ('imputer', SimpleImputer(strategy='constant', fill_value=2)),\n",
    "                                            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "                                        ])\n",
    "\n",
    "\n",
    "preprocessing_transformer4 = ColumnTransformer(transformers=[\n",
    "                                                                (\"numericPipe_log\", FunctionTransformer(np.log, validate=True), [\"policyHolderAge\", \"homeInsurancePremium\"]),\n",
    "                                                                (\"numericPipe\", StandardScaler(), num_features),\n",
    "                                                                ('genderPipe', OneHotEncoder(handle_unknown='ignore'), [\"Gender\"]),\n",
    "                                                                ('vehicleClaimPipe', OneHotEncoder(handle_unknown='ignore'), [\"hadVehicleClaimInPast\"]),\n",
    "                                                                ('hasMortgagePipe', impute_and_onehot_pipe, [\"hasMortgage\"]),\n",
    "                                                                ('vehicleStatusPipe', OneHotEncoder(handle_unknown='ignore'), [\"vehicleStatus\"]),\n",
    "                                                            ], remainder='passthrough')\n",
    "\n",
    "preprocessing_pipeline4 = Pipeline([\n",
    "                                      (\"terr_Encoder\", ce.CountEncoder(cols=[\"territory\"], return_df=True)),\n",
    "                                      (\"saleChannel_Encoder\", ce.CountEncoder(cols=[\"saleChannel\"], return_df=True)),\n",
    "                                      ('transform_column', preprocessing_transformer4)\n",
    "                                   ])\n",
    "preprocessing_pipeline4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transformer #5\n",
    "Imputation + Categorical Encodings + Numeric Standardization + Count Encoding (cat_long_features) + Normalization (cat_long_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "impute_and_onehot_pipe = Pipeline(steps=[\n",
    "                                            ('imputer', SimpleImputer(strategy='constant', fill_value=2)),\n",
    "                                            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "                                        ])\n",
    "\n",
    "\n",
    "preprocessing_transformer5 = ColumnTransformer(transformers=[                                        \n",
    "                                                                (\"numericPipe_log\", FunctionTransformer(np.log, validate=True), [\"policyHolderAge\", \"homeInsurancePremium\"]),\n",
    "                                                                (\"numericPipe\", StandardScaler(), num_features),\n",
    "                                                                (\"catLongPipe\", StandardScaler(), cat_long_features),\n",
    "                                                                ('genderPipe', OneHotEncoder(handle_unknown='ignore'), [\"Gender\"]),\n",
    "                                                                ('vehicleClaimPipe', OneHotEncoder(handle_unknown='ignore'), [\"hadVehicleClaimInPast\"]),\n",
    "                                                                ('hasMortgagePipe', impute_and_onehot_pipe, [\"hasMortgage\"]),\n",
    "                                                                ('vehicleStatusPipe', OneHotEncoder(handle_unknown='ignore'), [\"vehicleStatus\"]),\n",
    "                                                              ], remainder='passthrough')\n",
    "\n",
    "preprocessing_pipeline5 = Pipeline([\n",
    "                                        (\"terr_Encoder\", ce.CountEncoder(cols=[\"territory\"], return_df=True)),\n",
    "                                        (\"saleChannel_Encoder\", ce.CountEncoder(cols=[\"saleChannel\"], return_df=True)),\n",
    "                                        ('transform_column', preprocessing_transformer5)\n",
    "                                   ])\n",
    "\n",
    "preprocessing_pipeline5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transformer #6\n",
    "Imputation + Categorical Encodings + Numeric Standardization + Target Encoding (cat_long_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "impute_and_onehot_pipe = Pipeline(steps=[\n",
    "                                            ('imputer', SimpleImputer(strategy='constant', fill_value=2)),\n",
    "                                            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "                                        ])\n",
    "\n",
    "\n",
    "preprocessing_transformer6 = ColumnTransformer(transformers=[\n",
    "                                                                (\"numericPipe_log\", FunctionTransformer(np.log, validate=True), [\"policyHolderAge\", \"homeInsurancePremium\"]),\n",
    "                                                                (\"numericPipe\", StandardScaler(), num_features),\n",
    "                                                                ('genderPipe', OneHotEncoder(handle_unknown='ignore'), [\"Gender\"]),\n",
    "                                                                ('vehicleClaimPipe', OneHotEncoder(handle_unknown='ignore'), [\"hadVehicleClaimInPast\"]),\n",
    "                                                                ('hasMortgagePipe', impute_and_onehot_pipe, [\"hasMortgage\"]),\n",
    "                                                                ('vehicleStatusPipe', OneHotEncoder(handle_unknown='ignore'), [\"vehicleStatus\"]),\n",
    "                                                            ], remainder='passthrough')\n",
    "\n",
    "# You need to optimize the smoothing parameter through cross-validation.\n",
    "preprocessing_pipeline6 = Pipeline([\n",
    "                                      (\"terr_Encoder\", ce.TargetEncoder(cols=[\"territory\"], return_df=True)),\n",
    "                                      (\"saleChannel_Encoder\", ce.TargetEncoder(cols=[\"saleChannel\"], return_df=True)),\n",
    "                                      ('transform_column', preprocessing_transformer6)\n",
    "                                   ])\n",
    "preprocessing_pipeline6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transformer #7\n",
    "Imputation + Categorical Encodings + Numeric Standardization + Target Encoding (cat_long_features) + Normalization (cat_long_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "impute_and_onehot_pipe = Pipeline(steps=[\n",
    "                                            ('imputer', SimpleImputer(strategy='constant', fill_value=2)),\n",
    "                                            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "                                        ])\n",
    "\n",
    "\n",
    "preprocessing_transformer7 = ColumnTransformer(transformers=[\n",
    "                                                                (\"numericPipe_log\", FunctionTransformer(np.log, validate=True), [\"policyHolderAge\", \"homeInsurancePremium\"]),\n",
    "                                                                (\"numericPipe\", StandardScaler(), num_features),\n",
    "                                                                (\"catLongPipe\", StandardScaler(), cat_long_features),\n",
    "                                                                ('genderPipe', OneHotEncoder(handle_unknown='ignore'), [\"Gender\"]),\n",
    "                                                                ('vehicleClaimPipe', OneHotEncoder(handle_unknown='ignore'), [\"hadVehicleClaimInPast\"]),\n",
    "                                                                ('hasMortgagePipe', impute_and_onehot_pipe, [\"hasMortgage\"]),\n",
    "                                                                ('vehicleStatusPipe', OneHotEncoder(handle_unknown='ignore'), [\"vehicleStatus\"]),\n",
    "                                                              ], remainder='passthrough')\n",
    "\n",
    "preprocessing_pipeline7 = Pipeline([\n",
    "                                        (\"terr_Encoder\", ce.TargetEncoder(cols=[\"territory\"], return_df=True)),\n",
    "                                        (\"saleChannel_Encoder\", ce.TargetEncoder(cols=[\"saleChannel\"], return_df=True)),\n",
    "                                        ('transform_column', preprocessing_transformer7)\n",
    "                                   ])\n",
    "\n",
    "preprocessing_pipeline7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4] Prediction Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Import Evaluation Metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [4.1] Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Pipeline\n",
    "logRegPipe = Pipeline(\n",
    "                        steps=[\n",
    "                                    (\"preprocessing\", preprocessing_pipeline5),\n",
    "                                    (\"logReg\", LogisticRegression(random_state=10, class_weight=\"balanced\", solver=\"sag\"))\n",
    "                              ]\n",
    "                  )\n",
    "\n",
    "# Model Parameter Tuning\n",
    "logRegPipe_SS = [{\n",
    "                        'logReg__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "                        'logReg__C': [0.1, 1, 10, 100],\n",
    "                        'logReg__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "            }]\n",
    "logRegPipe_GCV = GridSearchCV(logRegPipe, logRegPipe_SS, cv=5, verbose=0, n_jobs=-1)\n",
    "\n",
    "# Evaluate Cross-validation Score\n",
    "RF_evalMeasures = cross_validate(logRegPipe_GCV, x_train, y_train, scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\" ])\n",
    "print(\"Evaluation Measures: \" + str(np.round(RF_evalMeasures, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logRegPipe_M = logRegPipe_GCV.fit(x_train, y_train)\n",
    "\n",
    "# Obtain predicted probabilities for the test data\n",
    "y_pred_proba = logRegPipe_GCV.predict_proba(x_test)\n",
    "\n",
    "# Compute the false positive rate, true positive rate, and thresholds for different probability cutoffs\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Compute the area under the curve (AUC)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Print Results\n",
    "print(\"tp Rate: \" + str(np.round(tpr.mean(), 3)))\n",
    "print(\"fp Rate: \" + str(np.round(fpr.mean(), 3)))\n",
    "print(\"AUC: \" + str(np.round(roc_auc, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Best Parameters\n",
    "logRegPipe_M.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [4.2] Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Pipeline\n",
    "RFPipe = Pipeline(\n",
    "                        steps=[\n",
    "                                    (\"preprocessing\", preprocessing_pipeline1),\n",
    "                                    (\"RF\", RandomForestClassifier(random_state=10, class_weight=\"balanced\"))\n",
    "                              ]\n",
    "                  )\n",
    "\n",
    "# Model Parameter Tuning\n",
    "RFPipe_SS = [{\n",
    "                        \"RF__n_estimators\": np.linspace(10, 500, 10, dtype=int),\n",
    "                        \"RF__max_depth\": np.array([3, 5, 7, 9, 11, 15, 20]),\n",
    "                        \"RF__max_features\": np.array([3, 5, 7, 9, 12, 15]),\n",
    "                        \"RF__min_samples_leaf\": np.linspace(5, 500, 10, dtype=int)               \n",
    "            }]\n",
    "RFPipe_GCV = GridSearchCV(RFPipe, RFPipe_SS, cv=5, verbose=0, n_jobs=-1)\n",
    "\n",
    "# Evaluate Cross-validation Score\n",
    "RF_evalMeasures = cross_validate(RFPipe_GCV, x_train, y_train, scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\" ])\n",
    "print(\"Evaluation Measures: \" + str(np.round(RF_evalMeasures, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFPipe_M = RFPipe_GCV.fit(x_train, y_train)\n",
    "\n",
    "# Obtain predicted probabilities for the test data\n",
    "y_pred_proba = RFPipe_GCV.predict_proba(x_test)\n",
    "\n",
    "# Compute the false positive rate, true positive rate, and thresholds for different probability cutoffs\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Compute the area under the curve (AUC)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Print Results\n",
    "print(\"tp Rate: \" + str(np.round(tpr, 3)))\n",
    "print(\"fp Rate: \" + str(np.round(fpr, 3)))\n",
    "print(\"AUC: \" + str(np.round(roc_auc, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Best Parameters\n",
    "RFPipe_GCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "importances = pd.DataFrame({'feature': preprocessing_transformer5.get_feature_names_out(), 'importance': RFPipe_M.feature_importances_})\n",
    "\n",
    "# Sort the features by importance in descending order\n",
    "importances = importances.sort_values('importance', ascending=False)\n",
    "\n",
    "# Print the top 10 most important features\n",
    "print(importances.head(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [4.3] SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Pipeline\n",
    "SVMPipe = Pipeline(\n",
    "                        steps=[\n",
    "                                    (\"preprocessing\", preprocessing_pipeline3),\n",
    "                                    (\"SVM\", SVC(random_state=10, class_weight=\"balanced\", kernel=\"rbf\"))\n",
    "                              ]\n",
    "                  )\n",
    "\n",
    "# Model Parameter Tuning\n",
    "SVMPipe_SS = [{\n",
    "                        \"SVM__C\": np.linspace(1, 100, 15),\n",
    "                        \"SVM__gamma\": np.linspace(1, 100, 10)\n",
    "                        \n",
    "            }]\n",
    "SVMPipe_GCV = GridSearchCV(SVMPipe, SVMPipe_SS, cv=5, verbose=0, n_jobs=-1)\n",
    "\n",
    "# Evaluate Cross-validation Score\n",
    "SVM_evalMeasures = cross_validate(SVMPipe_GCV, x_train, y_train, scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\" ])\n",
    "print(\"Evaluation Measures: \" + str(np.round(SVM_evalMeasures, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVMPipe_M = SVMPipe_GCV.fit(x_train, y_train)\n",
    "\n",
    "# Obtain predicted probabilities for the test data\n",
    "y_pred_proba = SVMPipe_GCV.predict_proba(x_test)\n",
    "\n",
    "# Compute the false positive rate, true positive rate, and thresholds for different probability cutoffs\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Compute the area under the curve (AUC)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Print Results\n",
    "print(\"tp Rate: \" + str(np.round(tpr, 3)))\n",
    "print(\"fp Rate: \" + str(np.round(fpr, 3)))\n",
    "print(\"AUC: \" + str(np.round(roc_auc, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Best Parameters\n",
    "SVMPipe_GCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5] Scoring Dataset Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "data_comp = pd.read_csv(\"ScoringDataset_2023Qualification.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Features\n",
    "x_comp = data_comp.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Results\n",
    "comp_predictions = RFPipe_GCV.predict_proba(x_comp)\n",
    "comp_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "# Convert probabilities into zeros and ones using the threshold value\n",
    "binary_predictions = np.where(comp_predictions > threshold, 1, 0)\n",
    "\n",
    "# Print the binary predictions\n",
    "binary_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to DF\n",
    "pred_df = pd.DataFrame(binary_predictions)\n",
    "pred_df.to_csv('comp_dataframe.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional, Tuple, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as fun\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import lightning as lt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1] Encoder-Decoder Transformer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.1] Positional Encoding Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding (nn.Module):\n",
    "    \"\"\"Positional encoding module with learnable parameters if needed.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model=512, max_len=5000, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Dimension of the model embeddings.\n",
    "            max_len: Maximum length of input sequences.\n",
    "            dropout: Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # pe(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
    "        # pe(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0)/d_model))       # Scale down sine/cosine frequencies\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)                                                          # ensure pe is not a learnable parameter\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
    "            \n",
    "        Returns:\n",
    "            Tensor with positional encoding added\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(1), :].unsqueeze(0)                                             # Add positional encoding to input embeddings\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.2] Multi-head Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention mechanism.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model=512, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model                                          # Total dimension of input/output embeddings.\n",
    "        self.num_heads = num_heads                                      # Number of attention heads.\n",
    "        self.d_k = d_model // num_heads                                 # Dimensionality per head.\n",
    "        \n",
    "        self.w_q = nn.Linear(d_model, d_model)                          # Define linear projects for w_q\n",
    "        self.w_k = nn.Linear(d_model, d_model)                          # Define linear projects for w_k\n",
    "        self.w_v = nn.Linear(d_model, d_model)                          # Define linear projects for w_v\n",
    "        self.w_o = nn.Linear(d_model, d_model)                          # Define linear projects for w_o\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.d_k]))          # Scaling to stabilize gradients.\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: Query tensor (batch_size, q_len, d_model)\n",
    "            key: Key tensor (batch_size, k_len, d_model)\n",
    "            value: Value tensor (batch_size, v_len, d_model)\n",
    "            mask: Optional mask tensor\n",
    "            \n",
    "        Returns:\n",
    "            Attention output tensor\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # 1. Apply Linear Projections\n",
    "        Q = self.w_q(query)                                             # (batch_size, q_len, d_model)\n",
    "        K = self.w_k(key)                                               # (batch_size, k_len, d_model)\n",
    "        V = self.w_v(value)                                             # (batch_size, v_len, d_model)\n",
    "        \n",
    "        # 2.Split into Multiple Heads\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # 3. Calculate Attention Scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale.to(query.device)\n",
    "        \n",
    "        # 4. Apply Masking\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            \n",
    "        # 5. Softmax and Dropout\n",
    "        attention = fun.softmax(scores, dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "        \n",
    "        # 6. Apply Attention Weights\n",
    "        x = torch.matmul(attention, V)\n",
    "        \n",
    "        # 7. Concatenate Heads\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # Final linear projection\n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.3] Encoder and Decoder Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1] Feedforward ANN\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feed forward network.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model=512, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)                             # Expand Dimension\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)                             # Project Data Back\n",
    "        self.dropout = nn.Dropout(dropout)                                  # Apply Regularization\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(fun.relu(self.linear1(x))))        # Apply Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2] Encoder Layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Single encoder layer with self-attention and feed forward.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model=512, num_heads=8, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # 1. Encoder Self-attention\n",
    "        attn_output = self.self_attn(x, x, x, mask)     # Encoder Self-attention\n",
    "        x = x + self.dropout1(attn_output)              # Residual Connection with Dropout Regularization\n",
    "        x = self.norm1(x)                               # Apply Normalization\n",
    "\n",
    "        # 2. Feedforward Layer\n",
    "        ff_output = self.feed_forward(x)                # Add Feedforward Layer\n",
    "        x = x + self.dropout2(ff_output)                # Residual Connection\n",
    "        x = self.norm2(x)                               # Apply Normalization\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [3] Decoder Layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"Single decoder layer with self-attention, encoder-decoder attention, and feed forward.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model=512, num_heads=8, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        # 1. Decoder Self-attention\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)                             # Decoder Self-attention\n",
    "        x = x + self.dropout1(attn_output)                                          # Residual Connection with Dropout Regularization\n",
    "        x = self.norm1(x)                                                           # Apply Normalization\n",
    "\n",
    "        # 2. Encoder-Decoder Attention\n",
    "        attn_output2 = self.enc_dec_attn(x, enc_output, enc_output, src_mask)       # Encoder Self-attention\n",
    "        x = x + self.dropout2(attn_output2)                                         # Residual Connection with Dropout Regularization\n",
    "        x = self.norm2(x)                                                           # Apply Normalization\n",
    "\n",
    "        # 3. Feedforward Layer\n",
    "        ff_output = self.feed_forward(x)                                            # Add Feedforward Layer\n",
    "        x = x + self.dropout3(ff_output)                                            # Residual Connection\n",
    "        x = self.norm3(x)                                                           # Apply Normalization\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.4] Encoder Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Transformer encoder with multiple layers.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model=512, num_layers=6, num_heads=8, d_ff=2048, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)                        # Define Token Embedding\n",
    "        self.position_encoding = PositionalEncoding(d_model, max_len, dropout)          # Define Positional Encoding\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout)    # Define Encoder Layers\n",
    "                                                            for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)                                              # Define Dropout Layer\n",
    "\n",
    "    def forward(self, src, mask=None):          \n",
    "        # 1. Embed Tokens and Add Positional Encoding\n",
    "        x = self.dropout(self.token_embedding(src))                                     # Token Embeddings\n",
    "        x = self.position_encoding(x)                                                   # Positional Encoding\n",
    "\n",
    "        # 2. Pass through Encoder Layers\n",
    "        for layer in self.layers:                                                       # Add Encoder Layers\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.5] Decoder Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Transformer decoder with multiple layers.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, d_model=512, num_layers=6, num_heads=8, d_ff=2048, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)                                # Define Toeken Embedding\n",
    "        self.position_encoding = PositionalEncoding(d_model, max_len, dropout)                  # Define Positional Encoding\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout)            # Define Decoder Layers\n",
    "                                                                for _ in range(num_layers)])\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, enc_output, src_mask=None, tgt_mask=None):\n",
    "        # 1. Embed Tokens and Positional Encoding\n",
    "        x = self.dropout(self.token_embedding(tgt))                                             # Token Embeddings\n",
    "        x = self.position_encoding(x)                                                           # Positional Encoding\n",
    "\n",
    "        # 2. Pass through Decoder Layers\n",
    "        for layer in self.layers:                                                               # Add Decoder Layers\n",
    "            x = layer(x, enc_output, src_mask, tgt_mask)\n",
    "        \n",
    "        # 3. Final Linear Layer\n",
    "        return self.fc_out(x)                                                                   # Add Final Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.6] Transformer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(lt.LightningModule):\n",
    "    \"\"\"Complete Transformer model for sequence-to-sequence tasks.\"\"\"\n",
    "\n",
    "    # 1. Initialization Function\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_layers=6, num_heads=8, d_ff=2048,\n",
    "                        dropout=0.1, max_len=5000, lr=0.0001, warmup_steps=4000):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.encoder = Encoder(src_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len)      # Define Encoder\n",
    "        self.decoder = Decoder(tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len)      # Define Decoder\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=0)                                                  # Define Loss Function\n",
    "        self.lr = lr        \n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "        # Initialize Parameters with Glorot Initialization\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "        \n",
    "    # 2. Forward Function\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        enc_output = self.encoder(src, src_mask)\n",
    "        output = self.decoder(tgt, enc_output, src_mask, tgt_mask)\n",
    "        return output\n",
    "    \n",
    "    # 3. Training Function\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        src, tgt = batch\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "\n",
    "        # Create Masks\n",
    "        src_mask = self._make_src_mask(src)\n",
    "        tgt_mask = self._make_tgt_mask(tgt_input)\n",
    "\n",
    "        # Forward Pass\n",
    "        output = self(src, tgt_input, src_mask, tgt_mask)\n",
    "\n",
    "        # Calculate Loss\n",
    "        loss = self.loss_fn(output.reshape(-1, output.shape[-1]), tgt_output.reshape(-1))\n",
    "\n",
    "        # Logging\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    # 4. Validation Function\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        src, tgt = batch\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "\n",
    "        # Create Masks\n",
    "        src_mask = self._make_src_mask(src)\n",
    "        tgt_mask = self._make_tgt_mask(tgt_input)\n",
    "\n",
    "        # Forward Pass\n",
    "        output = self(src, tgt_input, src_mask, tgt_mask)\n",
    "\n",
    "        # Calculate Loss\n",
    "        loss = self.loss_fn(output.reshape(-1, output.shape[-1]), tgt_output.reshape(-1))\n",
    "\n",
    "        # Logging\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "    \n",
    "    # 5. Optimizer Configuration Function\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=self.lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "        scheduler = {\n",
    "                        'scheduler': torch.optim.lr_scheduler.LambdaLR(\n",
    "                                                                        optimizer,\n",
    "                                                                        lr_lambda=lambda step:self._get_lr_multiplier(step)\n",
    "                                                                    ),\n",
    "                        'interval': 'step',\n",
    "                        'frequency': 1\n",
    "                    }\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    # 6. Learning Rate Warmup Function\n",
    "    def _get_lr_multiplier(self, step):\n",
    "        step = max(step, 1)                                                                     # Avoid division by zero\n",
    "        return min(step ** (-0.5), step * self.warmup_steps ** (-1.5))\n",
    "    \n",
    "    # 7. Source Sequences Mask Function\n",
    "    def _make_src_mask(self, src):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask\n",
    "    \n",
    "    # 8. Target Sequences Mask Function\n",
    "    def _make_tgt_mask(self, tgt):\n",
    "        tgt_pad_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_len = tgt.size(1)\n",
    "        tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()     # Lower Traingular Matrix\n",
    "        return tgt_pad_mask & tgt_sub_mask\n",
    "    \n",
    "    # 9. Generate Sequences Function\n",
    "    def generate(self, src, max_len=50, sos_idx=1, eos_idx=2):\n",
    "        self.eval()\n",
    "        with torch.no_grad():                                                       # Ensures Inference Mode\n",
    "            src_mask = self._make_src_mask(src)                                     # Create Mask for the Input\n",
    "            enc_output = self.encoder(src, src_mask)                                # Run Encoder on Source Input\n",
    "\n",
    "            # Initialize with SOS Token\n",
    "            tgt = torch.ones(src.size(0), 1).fill_(sos_idx).type_as(src)\n",
    "\n",
    "            for _ in range(max_len - 1):                                            # Generate One Token at a Time\n",
    "                tgt_mask = self._make_tgt_mask(tgt)                                 # Decoder Mask\n",
    "                output = self.decoder(tgt, enc_output, src_mask, tgt_mask)          # Run Decoder\n",
    "\n",
    "                # Get Last Predicted Token\n",
    "                next_token = output[:, -1, :].argmax(-1).unsqueeze(1)               # Get Next Token\n",
    "                tgt = torch.cat([tgt, next_token], dim=1)                           # Append Token to Sequences\n",
    "\n",
    "                # Stop - If all Sequences in Batch Predicted EOS\n",
    "                if (next_token==eos_idx).all():                                     # Early Stopping\n",
    "                    break\n",
    "        \n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.7] Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(src_vocab, tgt_vocab, batch_size=32, shuffle=True):\n",
    "    \"\"\"Create training and validation dataloaders.\"\"\"\n",
    "\n",
    "    train_inputs = torch.tensor(\n",
    "                                [\n",
    "                                    [\n",
    "                                        src_vocab['<SOS>'], src_vocab['i'], src_vocab['love'], src_vocab['spanish']\n",
    "                                    ],\n",
    "                                    \n",
    "                                    [\n",
    "                                        src_vocab['<SOS>'], src_vocab['i'], src_vocab['speak'], src_vocab['spanish']\n",
    "                                    ]\n",
    "                                ]\n",
    "                            )\n",
    "    train_outputs = torch.tensor(\n",
    "                                [\n",
    "                                    [\n",
    "                                        tgt_vocab['<SOS>'], tgt_vocab['yo'], tgt_vocab['amo'], tgt_vocab['espanol'], tgt_vocab['<EOS>']\n",
    "                                    ],\n",
    "\n",
    "                                    [\n",
    "                                        tgt_vocab['<SOS>'], tgt_vocab['yo'], tgt_vocab['hablo'], tgt_vocab['espanol'], tgt_vocab['<EOS>']\n",
    "                                    ]\n",
    "                                 ]\n",
    "                            )\n",
    "    \n",
    "    val_inputs = train_inputs.clone()\n",
    "    val_outputs = train_outputs.clone()\n",
    "\n",
    "    train_dataset = TensorDataset(train_inputs, train_outputs)\n",
    "    val_dataset = TensorDataset(val_inputs, val_outputs)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [2] Build the Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Vocabulary\n",
    "input_vocab = {\n",
    "                \"<PAD>\": 0,\n",
    "                \"<SOS>\": 1,\n",
    "                \"i\": 2,\n",
    "                \"love\": 3,\n",
    "                \"speak\": 4,\n",
    "                \"spanish\": 5\n",
    "            }\n",
    "output_vocab = {\n",
    "                \"<PAD>\": 0,\n",
    "                \"<SOS>\": 1,\n",
    "                \"<EOS>\": 2,\n",
    "                \"yo\": 3,\n",
    "                \"amo\": 4,\n",
    "                \"hablo\": 5,\n",
    "                \"espanol\": 6\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create Dataloaders\n",
    "train_loader, val_loader = create_dataloaders(input_vocab, output_vocab, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [3] Initialize and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize the Model\n",
    "model = Transformer(\n",
    "                        src_vocab_size=len(input_vocab),\n",
    "                        tgt_vocab_size=len(output_vocab),\n",
    "\n",
    "                        d_model=64,\n",
    "                        num_layers=1,\n",
    "                        num_heads=4,\n",
    "                        d_ff=512,\n",
    "\n",
    "                        dropout=0.1,\n",
    "                        max_len=10,\n",
    "                        lr=0.001,\n",
    "                        warmup_steps=20\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# 2. Train the Model\n",
    "trainer = lt.Trainer(\n",
    "                        max_epochs=150,\n",
    "                        accelerator='auto',\n",
    "                        devices=1 if torch.cuda.is_available() else None,\n",
    "                        enable_progress_bar=True,\n",
    "                        logger=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params | Mode\n",
      "----------------------------------------------------\n",
      "0 | encoder | Encoder          | 83.4 K | eval\n",
      "1 | decoder | Decoder          | 100 K  | eval\n",
      "2 | loss_fn | CrossEntropyLoss | 0      | eval\n",
      "----------------------------------------------------\n",
      "184 K     Trainable params\n",
      "0         Non-trainable params\n",
      "184 K     Total params\n",
      "0.736     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "52        Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149: 100%|██████████| 1/1 [00:00<00:00, 18.83it/s, v_num=55, train_loss=0.00105, val_loss=0.00104]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=150` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149: 100%|██████████| 1/1 [00:00<00:00, 11.22it/s, v_num=55, train_loss=0.00105, val_loss=0.00104]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output: tensor([[1, 3, 5, 6, 2]])\n"
     ]
    }
   ],
   "source": [
    "# Test generation\n",
    "test_input = torch.tensor([\n",
    "                            [\n",
    "                                input_vocab['<SOS>'], input_vocab['i'], input_vocab['speak'], input_vocab['spanish']\n",
    "                            ]\n",
    "    \n",
    "    ])\n",
    "\n",
    "\n",
    "output = model.generate(test_input, sos_idx=output_vocab['<SOS>'], eos_idx=output_vocab['<EOS>'])\n",
    "\n",
    "print(\"Generated output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sentence: yo hablo espanol\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create reverse vocabulary mapping\n",
    "output_id_to_token = {idx: token for token, idx in output_vocab.items()}\n",
    "\n",
    "# Step 2: Get the first (and only) sentence output\n",
    "output_tokens = output[0].tolist()\n",
    "\n",
    "# Step 3: Convert token IDs to words (excluding special tokens if desired)\n",
    "decoded_words = [output_id_to_token[token_id] for token_id in output_tokens if token_id not in {output_vocab['<SOS>'], output_vocab['<EOS>']}]\n",
    "\n",
    "# Step 4: Print\n",
    "print(\"Generated sentence:\", \" \".join(decoded_words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('pytorch_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12601079a068f98c8791dab2ee933b8ec92ba06a0f2447d32c8cfa2590f14d3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
